{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Sentiment Analysis of Amazon Reviews"]},{"cell_type":"markdown","metadata":{},"source":["![](https://www.topbots.com/wp-content/uploads/2020/01/cover_sentiment_analysis_BERT_1600px_web-1280x640.jpg)\n","\n","Hello Everyone!\n","\n","In this notebook, I’ll work with data from Amazon Review, which consists of 360000 reviews. There’re only positive and negative sentences.\n","\n","Steps:\n","* EDA\n","* Baseline Logistic Regression(Tf-Idf)\n","* DistilBert\n","* [DistilBert Inference Optimization](https://www.kaggle.com/alexalex02/nlp-transformers-inference-optimization)"]},{"cell_type":"markdown","metadata":{},"source":["## Importing libraries and reading data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import warnings\n","import seaborn as sns\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","import joblib\n","import eli5"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_val = pd.read_csv('../input/amazontrainreviews/train.csv', index_col=0)\n","train_val.reset_index(drop=True, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["# EDA"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(train_val.info())\n","display(train_val.head())"]},{"cell_type":"markdown","metadata":{},"source":["We have 0 Null value and now let's look at target distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sns.countplot(train_val['labels']);\n","plt.title('Labels distribution');"]},{"cell_type":"markdown","metadata":{},"source":["Let’s count number of words and see it distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_val['len'] = train_val['sentences'].apply(lambda x: len(x.split()))\n","sns.distplot(train_val['len']);"]},{"cell_type":"markdown","metadata":{},"source":["Now we’ll divide it by sentiment and calculate average values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["neg_mean_len = train_val.groupby('labels')['len'].mean().values[0]\n","pos_mean_len = train_val.groupby('labels')['len'].mean().values[1]\n","\n","print(f\"Negative mean length: {neg_mean_len:.2f}\")\n","print(f\"Positive mean length: {pos_mean_len:.2f}\")\n","print(f\"Mean Difference: {neg_mean_len-pos_mean_len:.2f}\")\n","ax = sns.catplot(x='labels', y='len', data=train_val, kind='box')"]},{"cell_type":"markdown","metadata":{},"source":["We can see that negative sentences are longer on average. To say how significant this difference, we use permutation testing and calculate p-value.\n","\n","First, we define a function to generate a permutation sample from two arrays. Then, we generate permutation replicates, which are a single statistic computed from permutation sample. Last, we compute the probability of getting at least 5.91 difference in mean under the hypothesis that the distributions of words are identical."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["neg_array = train_val[train_val['labels']==0]['len'].values\n","pos_array = train_val[train_val['labels']==1]['len'].values\n","mean_diff = neg_mean_len - pos_mean_len"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def permutation_sample(data1, data2):\n","    # Permute the concatenated array: permuted_data\n","    data = np.concatenate((data1,data2))\n","    permuted_data = np.random.permutation(data)\n","\n","    # Split the permuted array into two: perm_sample_1, perm_sample_2\n","    perm_sample_1 = permuted_data[:len(data1)]\n","    perm_sample_2 = permuted_data[len(data1):]\n","\n","    return perm_sample_1, perm_sample_2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def draw_perm_reps(data_1, data_2, size=1):\n","\n","    perm_replicates = np.empty(size)\n","\n","    for i in range(size):\n","        # Generate permutation sample\n","        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)\n","\n","        # Compute the test statistic\n","        perm_replicates[i] = np.mean(perm_sample_1) - np.mean(perm_sample_2)\n","\n","    return perm_replicates"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["perm_replicates = draw_perm_reps(neg_array, pos_array,\n","                                 size=10000)\n","\n","# Compute p-value: p\n","p = np.sum(perm_replicates >= mean_diff) / len(perm_replicates)\n","\n","print(f'p-value = {p}')"]},{"cell_type":"markdown","metadata":{},"source":["The p-value tells us that the null hypothesis is false."]},{"cell_type":"markdown","metadata":{},"source":["# Baseline - LogReg (Tf-Idf)"]},{"cell_type":"markdown","metadata":{},"source":["Our baseline will be Logistic Regression with Tf-Idf. First, we define a function for prediction, which calculates accuracy, f1_score, confusion matrix and saves our model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def prediction(model, X_train, y_train, X_valid, y_valid):\n","    model.fit(X_train, y_train)\n","    pred = model.predict(X_valid)\n","    acc = accuracy_score(y_valid, pred)\n","    f1 = f1_score(y_valid, pred)\n","    conf = confusion_matrix(y_valid, pred)\n","    joblib.dump(model, f\"model_acc_{acc:.5f}.pkl\")\n","    return model, acc, f1, conf"]},{"cell_type":"markdown","metadata":{},"source":["Extracting unigrams, bigrams and trigrams, also removing stopwords."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["transformer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3), \n","                              lowercase=True, max_features=100000)\n","X = transformer.fit_transform(train_val['sentences'])\n","y = train_val.labels"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, \n","                                                      random_state=42, stratify=y)\n","model = LogisticRegression(C=1, random_state=42, n_jobs=-1)\n","fit_model, acc, f1, conf = prediction(model, X_train, y_train, X_valid, y_valid)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f\"Accuracy: {acc:.5f}\")\n","print(f\"F1_Score: {f1:.5f}\")\n","print(f\"Confusion Matrix: {conf}\")"]},{"cell_type":"markdown","metadata":{},"source":["Interpreting model weights with ELI5."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["eli5.show_weights(estimator=fit_model, \n","                  feature_names= list(transformer.get_feature_names()),\n","                    top=(20,20))"]},{"cell_type":"markdown","metadata":{},"source":["# DistilBert"]},{"cell_type":"markdown","metadata":{},"source":["Here we'll use DistilBert from [transformers](https://huggingface.co/transformers/index.html). And [catalyst](https://github.com/catalyst-team/catalyst) for running experiment.\n","\n","First, we install torch nightly for Mixed-precision training."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["!pip install --pre torch==1.7.0.dev20200701+cu101 torchvision==0.8.0.dev20200701+cu101 -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n","import torch\n","torch.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","os.environ['WANDB_SILENT'] = 'True'\n","os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n","\n","from typing import Mapping, List\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","\n","from transformers import AutoConfig, AutoTokenizer, AutoModel\n","\n","from catalyst.dl import SupervisedRunner\n","from catalyst.dl.callbacks import AccuracyCallback, OptimizerCallback, CheckpointCallback, WandbLogger\n","from catalyst.utils import set_global_seed, prepare_cudnn\n","from catalyst.contrib.nn import RAdam, Lookahead, OneCycleLRWithWarmup\n","import wandb"]},{"cell_type":"markdown","metadata":{},"source":["Config setup"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["MODEL_NAME = 'distilbert-base-uncased'\n","LOG_DIR = \"./amazon\" \n","NUM_EPOCHS = 2 \n","LEARNING_RATE = 5e-5\n","MAX_SEQ_LENGTH = 512\n","BATCH_SIZE = 32\n","WEIGHT_DECAY = 1e-3\n","ACCUMULATION_STEPS = 3\n","SEED = 42\n","FP_16 = dict(opt_level=\"O1\")"]},{"cell_type":"markdown","metadata":{},"source":["For reproducibility"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["set_global_seed(SEED)\n","prepare_cudnn(deterministic=True, benchmark=True)"]},{"cell_type":"markdown","metadata":{},"source":["We'll create dataset. Instantiate tokenizer. Then, we convert tokens to integers, add special tokens, use padding to max_length. Return `'input_ids', 'attention_mask', 'targets'`"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ReviewDataset(Dataset):\n","\n","    \n","    def __init__(self,\n","                 sentences: List[str],\n","                 labels: List[str] = None,\n","                 max_seq_length: int = MAX_SEQ_LENGTH,\n","                 model_name: str = 'distilbert-base-uncased'):\n","\n","        self.sentences = sentences\n","        self.labels = labels\n","        self.max_seq_length = max_seq_length\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","        \n","    def __len__(self):\n","\n","        return len(self.sentences)\n","\n","    \n","    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n","\n","        sentence = self.sentences[index]\n","        encoded = self.tokenizer.encode_plus(sentence, add_special_tokens=True, \n","                                        pad_to_max_length=True, max_length=self.max_seq_length, \n","                                        return_tensors=\"pt\",)\n","        \n","        output = {\n","            'input_ids': encoded['input_ids'],\n","            'attention_mask': encoded['attention_mask']\n","        }\n","        \n","        output['targets'] = torch.tensor(self.labels[index], dtype=torch.long)\n","        \n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["Making train_test_split, defining datasets and loaders"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train, df_valid = train_test_split(\n","            train_val,\n","            test_size=0.2,\n","            random_state=42,\n","            stratify = train_val.labels.values\n","        )\n","print(df_train.shape, df_valid.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["train_dataset = ReviewDataset(\n","    sentences=df_train['sentences'].values.tolist(),\n","    labels=df_train['labels'].values,\n","    max_seq_length=MAX_SEQ_LENGTH,\n","    model_name=MODEL_NAME\n",")\n","\n","valid_dataset = ReviewDataset(\n","    sentences=df_valid['sentences'].values.tolist(),\n","    labels=df_valid['labels'].values,\n","    max_seq_length=MAX_SEQ_LENGTH,\n","    model_name=MODEL_NAME\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_val_loaders = {\n","    \"train\": DataLoader(dataset=train_dataset,\n","                        batch_size=BATCH_SIZE, \n","                        shuffle=True, num_workers=2, pin_memory=True),\n","    \"valid\": DataLoader(dataset=valid_dataset,\n","                        batch_size=BATCH_SIZE, \n","                        shuffle=False, num_workers=2, pin_memory=True)    \n","}"]},{"cell_type":"markdown","metadata":{},"source":["Review and model input"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(df_valid.sentences.values[50])\n","valid_dataset[50]"]},{"cell_type":"markdown","metadata":{},"source":["Initialize pre-trained model. From config we'll use dimensionality of the encoder layers and the pooler layer = 768. And dropout probabilities = 0.2. Then, we'll compute logits for the input sequence."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class DistilBert(nn.Module):\n","\n","    def __init__(self, pretrained_model_name: str = MODEL_NAME, num_classes: int = 2):\n","\n","        super().__init__()\n","\n","        config = AutoConfig.from_pretrained(\n","             pretrained_model_name)\n","\n","        self.distilbert = AutoModel.from_pretrained(pretrained_model_name,\n","                                                    config=config)\n","        self.pre_classifier = nn.Linear(config.dim, config.dim)\n","        self.classifier = nn.Linear(config.dim, num_classes)\n","        self.dropout = nn.Dropout(config.seq_classif_dropout)\n","\n","    def forward(self, input_ids, attention_mask=None, head_mask=None):\n","\n","        assert attention_mask is not None, \"attention mask is none\"\n","        distilbert_output = self.distilbert(input_ids=input_ids,\n","                                            attention_mask=attention_mask,\n","                                            head_mask=head_mask)\n","        hidden_state = distilbert_output[0]  # [BATCH_SIZE=32, MAX_SEQ_LENGTH = 512, DIM = 768]\n","        pooled_output = hidden_state[:, 0]  # [32, 768]\n","        pooled_output = self.pre_classifier(pooled_output)  # [32, 768]\n","        pooled_output = F.relu(pooled_output)  # [32, 768]\n","        pooled_output = self.dropout(pooled_output)  # [32, 768]\n","        logits = self.classifier(pooled_output)  # [32, 2]\n","\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["model = DistilBert()"]},{"cell_type":"markdown","metadata":{},"source":["Training setup:\n","\n","1. We applied weight decay for all parameters except 'bias' and 'LayerNorm'\n","1. Lookahead optimizer(improves the learning stability and lowers the variance of its inner optimizer)\n","1. OneCycleLRWithWarmup with 0 warmup steps, cosine annealing from 5e-5 to 1e-8.\n","1. Gradient accumulation for large batch training."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["param_optim = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","\n","base_optimizer = RAdam([\n","    {'params': [p for n,p in param_optim if not any(nd in n for nd in no_decay)],\n","     'weight_decay': WEIGHT_DECAY}, \n","    {'params': [p for n,p in param_optim if any(nd in n for nd in no_decay)],\n","     'weight_decay': 0.0}\n","])\n","optimizer = Lookahead(base_optimizer)\n","scheduler = OneCycleLRWithWarmup(\n","    optimizer, \n","    num_steps=NUM_EPOCHS, \n","    lr_range=(LEARNING_RATE, 1e-8),\n","    init_lr=LEARNING_RATE,\n","    warmup_steps=0,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["runner = SupervisedRunner(\n","    input_key=(\n","        \"input_ids\",\n","        \"attention_mask\"\n","    )\n",")\n","# model training\n","runner.train(\n","    model=model,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    loaders=train_val_loaders,\n","    callbacks=[\n","        AccuracyCallback(num_classes=2),\n","        OptimizerCallback(accumulation_steps=ACCUMULATION_STEPS),\n","        WandbLogger(name=\"Name\", project=\"sentiment-analysis\"),\n","    ],\n","    fp16=FP_16,\n","    logdir=LOG_DIR,\n","    num_epochs=NUM_EPOCHS,\n","    verbose=True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["![](https://i.ibb.co/9wxK0Zz/Val-Metric.png)"]},{"cell_type":"markdown","metadata":{"trusted":true},"source":["After two epochs, we’ll able to reach 96.22% accuracy, which is on 6% higher than logistic regression.\n","\n","To improve our result even more, we can continue fine-tuning with frozen encoder.\n","\n","\n","### Test"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","def prediction(model, sentence: str, max_len: int = 512, device = 'cpu'):\n","    x_encoded = tokenizer.encode_plus(sentence, add_special_tokens=True, pad_to_max_length=True, max_length=max_len, return_tensors=\"pt\",).to(device)\n","    logits = model(x_encoded['input_ids'], x_encoded['attention_mask'])\n","    probabilities = F.softmax(logits.detach(), dim=1)\n","    output = probabilities.max(axis=1)\n","    print(sentence)\n","    print(f\"Class: {['Negative' if output.indices[0] == 0 else 'Positive'][0]}, Probability: {output.values[0]:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prediction(plain_model, df_valid['sentences'].values[20])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
