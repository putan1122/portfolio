# Pu Tan
#### Contact: [putan@seas.upenn.edu](mailto:putan@seas.upenn.edu) | (445) 800-6900 | [LinkedIn](https://www.linkedin.com/in/pu-tan1122)

## Technical Skills
Programming: Java, Python, SQL, R, JavaScript, MATLAB  
Analytics & Tools: MySQL, Anaconda, Apache Spark, Tableau, Visual Studio, Microsoft Office Suite (Excel), Oracle, DataGrip, Github, AWS  
Libraries: NumPy, Pandas, SciPy, scikit-learn, matplotlib, Plotly, Seaborn, Biogeme

## Education
- Master of Science in Engineering in Data Science | University of Pennsylvania (_Expected May 2024_)  
  - GPA: 3.80
- Bachelor of Science in Probability and Statistics; Bachelor of Arts in Economics | University of California, San Diego (_Sept 2018 - June 2022_)  
  - GPA: 3.92

### Main Courses
Computational Statistics and Data Analysis, Statistical Inference, Stochastic Processes, Applied Machine Learning, Data Structure and Algorithms, Databases & Information System, Internet and Web Systems, Natural Language Processing (Computational Linguistics)

## Professional Experience

**Machine Learning Engineer Intern, Metallurgical Group Corporation (Data Technology Institute), Mainland, China (_Jun. 2023 - Aug.2023_)**
- Employed traditional statistical models and fine-tuning on pre-trained deep learning models to achieve time-series forecasting performance using PyTorch on 312 distinct steel smelting process-related data features, enhancing prediction accuracy by 8% over standard models
- Designed and executed novel algorithm in a Jupyter Notebook environment to capture and predict temporal dependencies in multivariate time-series forecasting
- Utilized the fine-tuned models to predict the quality of smelting materials, captured non-linear data patterns in smelting process, converting complex SQL queries and analyses into actionable insights, enhancing the strategic decision-making process in machinery investments
- Assisted in managing data pipelines by working closely with backend engineers to create an API for integrating deep learning model into big data production system
- Drove advancements in long-term forecasting and anomaly detection capabilities, shared insightful findings through Tableau dashboards



**Data Analyst Intern, San Diego Supercomputer Center (SDSC) Research Data Services, San Diego, CA (_Mar. 2022 - Jun. 2022_)**
- Conducted hypothesis testing on 6 market cloud services to identify key instances; optimized existing strategies on the identified instances, e.g., placement, marketing and budgeting for external clients based on historical usage data
- Built the Dynamics CRM system via complex SQL queries on 650+ unstructured data records to automatically identify discrepancies; developed interactive Tableau dashboards to track sales key metrics, improving 60% efficiency
- Implemented SVM to predicted Cloud Compute demand and achieved 98.1% accuracy; found key factors on clientâ€™s trade flows and summarized the results with 25-page reports via Plotly 
- Designed and conducted A/B testing to explore the provided optimization methods on data-preprocessing for recommendation message system, which increased the conversion rate by 2%; developed the data-preprocessing pipeline on 600+ Public Cloud Services accounts and invoices using Python; reduced 5 labor hours per week
- Developed financial dashboards to track 5 key KPIs (e.g. churn rate, net profit) to communicate demand trend and share actionable decisions with business leadership for next business decision making process, receiving 100% positive feedback


**Data Analyst Intern, Shepherd Ventures, Inc., San Diego, CA (_Apr. 2021 - Sep. 2021_)**
- Led stock selection project and implemented a Genetic Algorithm to generate initial stock combinations; optimized the stock combinations via logistic regression with hyper-parameter tuning for stock price prediction which achieved 0.0115 MAE
- Leveraged advanced Excel functionalities, such as pivot tables and conditional lookups, to analyze and interpret complex datasets
- Conducted significance testing between the results generated by equal allocation policy and minimal risk strategy with neural network (NN); verified NN can significantly improve prediction by 8% MPE and improve the profits
- Evaluated portfolios by performing Fama's decomposition, examining time plots of the performance of all portfolios benchmarked against S&P 500, and calculating 99% 5-day VaR from Monte Carlo simulations



## Project Experience

### BookPilot Web Application (_Jan. 2023 - May 2023_) 
- Led the development and quality assurance of BookPilot, a web application aimed at streamlining the process of exploring book,author,review
- Conducted extensive data cleaning and normalization on a large dataset of over 212,000 unique books, leveraged B and B+ Trees for optimizing data retrieval operations
- Created and optimized complex SQL queries in Oracle for data retrieval, improving query performance by 70%
- Developed and executed over 150 test cases, both automated and manual, ensuring the reliability and efficiency of features like user login, book and author search, and genre ranking
- Ensured the integrity and efficiency of the database by maintaining it in the Third Normal Form (3NF) for eliminating data redundancies
- Employed XQuery and XML Schemas in data integration processes, enhanced the application's data handling capabilities
- Developed a highly responsive front-end interface using a combination of HTML, CSS, JavaScript, and Node.js, enhancing user engagement significantly
- Utilized the React framework for building the web application, allowing efficient state management and the rendering of components, which led to an improved user experience
- Implemented a user login feature with integrated Google and Twitter sign-in options, employing Firebase for user authentication and data security



### COVID-19 Vaccines Distribution Policy Design with Reinforcement Learning (_Jun. 2021 - Sep. 2021_)
-Researched and developed linear regression, random forest, and AdaBoost models to estimate confirmed COVID-19 cases in San Diego; implemented deep Q-network (DQN) model with the estimated number of cases to distribute three types of vaccines, decreasing confirmed cases by 1.9%  
Designed a prediction + optimization framework consisting of random forest and DQN to derive the optimal distribution strategy; summarized the proposed method as a first-author research paper which was published in 2021 4th International Conference on Big Data and Machine Learning (Paper ID: BP2103)  [Read the paper here]([https://drive.google.com/file/d/1ig61UdLxVR-UuEGop_c0rCfp4_tZ4W5c/view?usp=sharing])





### Recommender Systems Design & Five ML Algorithms Implementation (_Sep. 2021 - Nov. 2021_)
- Investigated and evaluated machine learning algorithms on Amazon recommendation datasets; designed and developed quantitative analysis system to extract samples from datasets with 255,404 users and items
- Implemented three classical Machine Learning algorithms, e.g., BPRMF, ItemKNN; optimized the classical methods with two state-of-the-art deep learning methods, such as DeepFM and LightGCN with 17% improvement in recall and 11% in Normalized Discounted Cumulative Gain (NDCG)
- Performed hyper-parameter tuning with 10-fold cross-validation, achieving 75% recall and 56% NDCG



### Sentiment Analysis for Marketing via NLP (_Sep. 2022 - Nov. 2022_)                                                                     
- Applied NLPK to process and classify the language text and performed exploratory data analysis on IMDB social media comment and Amazon review with Pandas in Python, ELT processing with SQL queries in Spark
- Analyzing the reviews with a state-of-the-art deep learning technique, BERT and DistilBERT; evaluated DistilBERT on predicting the positive sentiment assessment and achieved 66% recall, 60% precision


## Publications & Competition

- [Data Science Blog](https://medium.com/@shawhin)

- **Predicting Medical Conditions with InVitro Cell Research (Kaggle Ranked Top 4%)** (_May 2023 - Aug. 2023_)
  - Identified age-related features in anonymized data, improving balanced log-loss scores
  - Applied robust feature engineering techniques to ensure model stability and scalability on limited datasets


